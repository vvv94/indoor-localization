{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "localization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrNo1sNstAGplnI7HdQnMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvv94/indoor-localization/blob/main/localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cblM3xFjcnln"
      },
      "source": [
        "Installing required Packages and import Python Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpVyNWaZZfsE",
        "outputId": "441a740b-b087-458b-cc07-86b45fb77065"
      },
      "source": [
        "!apt -qq -y install unzip >/dev/null 2>&1\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from warnings import simplefilter\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn import model_selection\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (50, 30)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEum84dOccHb"
      },
      "source": [
        "Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUIqo_7WWyk"
      },
      "source": [
        "!rm -r sample_data dataset README.rtf >/dev/null 2>&1\n",
        "!wget https://www.utwente.nl/en/eemcs/ps/dataset-folder/soloc-ipin2017-dataset.zip -q \n",
        "!unzip soloc-ipin2017-dataset.zip >/dev/null 2>&1\n",
        "!rm -r __MACOSX >/dev/null 2>&1\n",
        "!mv ./SoLoc_IPIN2017_dataset ./dataset >/dev/null 2>&1\n",
        "!rm *.zip >/dev/null 2>&1\n",
        "!mv ./dataset/readme.rtf ./README.rtf >/dev/null 2>&1\n",
        "!mkdir -p results >/dev/null 2>&1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkAtRY8cuzA"
      },
      "source": [
        "Load Dataset into a Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdU2kQOIb0JU"
      },
      "source": [
        "'''Project Parameters'''\n",
        "DATASET_DIR = \"./dataset/\"\n",
        "RESULTS_DIR = \"./results\"\n",
        "DATASET_NAMES = [   \n",
        "'P_SA',             # RSS values between the smartphone and 11 WiFi APs\n",
        "'P_SA_Signatures',  # RSS values between the smartphone and 11 WiFi APs for calibration points (signatures)\n",
        "'P_SA_Tests',       # RSS values between the smartphone and 11 WiFi APs for test points\n",
        "'APLocs',           # x and y coordinates of 11 APs with RSS values stored in P_SA\n",
        "\n",
        "'P_SB',             # RSS values between the smartphone and 11 WiFi APs\n",
        "'P_SB_Signatures',  # RSS values between the smartphone and 46 Bluetooth beacons for calibration points (signatures)\n",
        "'P_SB_Tests',       # RSS values between the smartphone and 46 Bluetooth beacons for test points\n",
        "'BeaconLocs',       # x and y coordinates of 46 Bluetooth beacons with RSS values stored in P_SB\n",
        "\n",
        "'MeasLocs',         # x and y coordinates of the smartphone with RSS values stored in P_SA and P_SB\n",
        "'SignatureLocs',    # x and y coordinates of calibration points (signatures) with RSS values stored in P_SA_Signature and P_SB_Signature\n",
        "'TestLocs',         # x and y coordinates of test points with RSS values stored in P_SA_Tests and P_SB_Tests\n",
        "'P_Signatures',     # combination of RSS values for calibration points (signatures) - P_SA_Signature and P_SB_Signature\n",
        "'P_Tests',          # combination of RSS values for test points - P_SA_Tests and P_SB_Tests\n",
        "]\n",
        "\n",
        "def pre_process(data_name):\n",
        "\n",
        "  _colnames = []\n",
        "  if data_name == \"P_SA\":\n",
        "    _colnames=['WF1 RSS', 'WF2 RSS', 'WF3 RSS', 'WF4 RSS','WF5 RSS', 'WF6 RSS', 'WF7 RSS', 'WF8 RSS','WF9 RSS', 'WF10 RSS', 'WF11 RSS']\n",
        "    data = pd.read_csv(\"./dataset/P_SA.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SA_Signatures\":\n",
        "    _colnames=['WF1 RSS Sig.', 'WF2 RSS Sig.', 'WF3 RSS Sig.', 'WF4 RSS Sig.','WF5 RSS Sig.', 'WF6 RSS Sig.', 'WF7 RSS Sig.', 'WF8 RSS Sig.','WF9 RSS Sig.', 'WF10 RSS Sig.', 'WF11 RSS Sig.']\n",
        "    data = pd.read_csv(\"./dataset/P_SA_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SA_Tests\":\n",
        "    _colnames=['WF1 RSS Test', 'WF2 RSS Test', 'WF3 RSS Test', 'WF4 RSS Test','WF5 RSS Test', 'WF6 RSS Test', 'WF7 RSS Test', 'WF8 RSS Test','WF9 RSS Test', 'WF10 RSS Test', 'WF11 RSS Test']\n",
        "    data = pd.read_csv(\"./dataset/P_SA_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"APLocs\":\n",
        "    _colnames=['AP Coordinates', 'RSS Value in P_SA']\n",
        "    data = pd.read_csv(\"./dataset/APLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB_Signatures\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS Sig.') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB_Tests\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS Test') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"BeaconLocs\":\n",
        "    _colnames=['BT Coordinates', 'RSS Value in P_SB']\n",
        "    data = pd.read_csv(\"./dataset/BeaconLocs.csv\", sep='\\t',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"MeasLocs\":\n",
        "    _colnames=['Smartphone Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/MeasLocs.csv\", sep='\t',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"SignatureLocs\":\n",
        "    _colnames=['Calibration Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/SignatureLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"TestLocs\":\n",
        "    _colnames=['Test Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/TestLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_Signatures\":\n",
        "    [_colnames.append('WF'+str(i)+' RSS Sign.') for i in range(1,12)]\n",
        "    [_colnames.append('BT'+str(i)+' RSS Sign.') for i in range(12,58)]\n",
        "    data = pd.read_csv(\"./dataset/P_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_Tests\":\n",
        "    [_colnames.append('WF'+str(i)+' RSS Test') for i in range(1,12)]\n",
        "    [_colnames.append('BT'+str(i)+' RSS Test') for i in range(12,58)]\n",
        "    data = pd.read_csv(\"./dataset/P_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  else:\n",
        "    data = []\n",
        "\n",
        "  return data\n",
        "\n",
        "data = [pre_process(name) for idx, name in enumerate(DATASET_NAMES)]\n",
        "dataset1 = [data[0], data[1], data[2], data[3]]\n",
        "dataset2 = [data[4], data[5], data[6], data[7]]\n",
        "dataset3 = [data[8], data[9], data[10], data[11], data[12]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340Y50V2a6y6"
      },
      "source": [
        "Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsW7s-LaUC_3"
      },
      "source": [
        "dataset1[0].insert(loc=0,column=\"Points\",value=list(range(603)))\n",
        "# ['Poitns, WF1 RSS', 'WF2 RSS', 'WF3 RSS', 'WF4 RSS','WF5 RSS', 'WF6 RSS', 'WF7 RSS', 'WF8 RSS','WF9 RSS', 'WF10 RSS', 'WF11 RSS']\n",
        "dataset1[2].insert(loc=0,column=\"Points\",value=list(range(475)))\n",
        "# ['Poitns, WF1 RSS Test', 'WF2 RSS Test', 'WF3 RSS Test', 'WF4 RSS Test','WF5 RSS Test', 'WF6 RSS Test', 'WF7 RSS Test', 'WF8 RSS Test','WF9 RSS Test', 'WF10 RSS Test', 'WF11 RSS Test']\n",
        "\n",
        "#dataset1[0].plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False)\n",
        "#dataset1[0].hist()\n",
        "#scatter_matrix(dataset1[0])\n",
        "#plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j5u5VwAh_vF"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "train_set = shuffle(dataset1[0].values, random_state=8)\n",
        "test_set = shuffle(dataset1[2].values, random_state=8)\n",
        "train_values = train_set[:, 1:6]\n",
        "train_labels = train_set[:, 0]\n",
        "test_values = test_set[:, 1:6]\n",
        "test_labels = test_set[:, 0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7naxrBXsdO0y"
      },
      "source": [
        "#print(train_values)\n",
        "#print(train_labels)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGCEk_GoilVo",
        "outputId": "e59b21ec-0957-471d-dc73-13fefdc325fc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from warnings import simplefilter\n",
        "from sklearn import model_selection\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(max_iter=1000)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "scoring = 'accuracy'\n",
        "\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "for name, model in models:\n",
        "\n",
        "    kfold = model_selection.KFold(n_splits=10)\n",
        "    cv_results = model_selection.cross_val_score(model, train_values, train_labels, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(train_values, train_labels)\n",
        "predictions = knn.predict(test_values)\n",
        "\n",
        "print()\n",
        "print(\"predictions result :\")\n",
        "print(\"KNN accuracy score : \",accuracy_score(test_labels, predictions))\n",
        "print(\"confusion matrix : \")\n",
        "print(confusion_matrix(test_labels, predictions))\n",
        "print()\n",
        "print(\"classification report : \")\n",
        "print(classification_report(test_labels, predictions))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: The number of samples must be more than the number of classes.\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR: 0.000000 (0.000000)\n",
            "LDA: nan (nan)\n",
            "KNN: 0.000000 (0.000000)\n",
            "CART: 0.000000 (0.000000)\n",
            "NB: 0.000000 (0.000000)\n",
            "SVM: 0.000000 (0.000000)\n",
            "\n",
            "predictions result :\n",
            "KNN accuracy score :  0.002105263157894737\n",
            "confusion matrix : \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "\n",
            "classification report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00         1\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         1\n",
            "          14       0.00      0.00      0.00         1\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.00      0.00      0.00         1\n",
            "          30       0.00      0.00      0.00         1\n",
            "          31       0.00      0.00      0.00         1\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.00      0.00      0.00         1\n",
            "          34       0.00      0.00      0.00         1\n",
            "          35       0.00      0.00      0.00         1\n",
            "          36       0.00      0.00      0.00         1\n",
            "          37       0.00      0.00      0.00         1\n",
            "          38       0.00      0.00      0.00         1\n",
            "          39       0.00      0.00      0.00         1\n",
            "          40       0.00      0.00      0.00         1\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          43       0.00      0.00      0.00         1\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         1\n",
            "          47       0.00      0.00      0.00         1\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.00      0.00      0.00         1\n",
            "          53       0.00      0.00      0.00         1\n",
            "          54       0.00      0.00      0.00         1\n",
            "          55       0.00      0.00      0.00         1\n",
            "          56       0.00      0.00      0.00         1\n",
            "          57       0.00      0.00      0.00         1\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.00      0.00      0.00         1\n",
            "          60       0.00      0.00      0.00         1\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         1\n",
            "          65       0.00      0.00      0.00         1\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00         1\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.00      0.00      0.00         1\n",
            "          70       0.00      0.00      0.00         1\n",
            "          71       0.00      0.00      0.00         1\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00         1\n",
            "          75       0.00      0.00      0.00         1\n",
            "          76       0.00      0.00      0.00         1\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.00      0.00      0.00         1\n",
            "          80       0.00      0.00      0.00         1\n",
            "          81       0.00      0.00      0.00         1\n",
            "          82       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00         1\n",
            "          86       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         1\n",
            "          88       0.00      0.00      0.00         1\n",
            "          89       0.00      0.00      0.00         1\n",
            "          90       0.00      0.00      0.00         1\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         1\n",
            "          94       0.00      0.00      0.00         1\n",
            "          95       0.00      0.00      0.00         1\n",
            "          96       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         1\n",
            "          98       0.00      0.00      0.00         1\n",
            "          99       0.00      0.00      0.00         1\n",
            "         100       0.00      0.00      0.00         1\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         1\n",
            "         104       0.00      0.00      0.00         1\n",
            "         105       0.00      0.00      0.00         1\n",
            "         106       0.00      0.00      0.00         1\n",
            "         107       0.00      0.00      0.00         1\n",
            "         108       0.00      0.00      0.00         1\n",
            "         109       0.00      0.00      0.00         1\n",
            "         110       0.00      0.00      0.00         1\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00         1\n",
            "         113       0.00      0.00      0.00         1\n",
            "         114       0.00      0.00      0.00         1\n",
            "         115       0.00      0.00      0.00         1\n",
            "         116       0.00      0.00      0.00         1\n",
            "         117       0.00      0.00      0.00         1\n",
            "         118       0.00      0.00      0.00         1\n",
            "         119       0.00      0.00      0.00         1\n",
            "         120       0.00      0.00      0.00         1\n",
            "         121       0.00      0.00      0.00         1\n",
            "         122       0.00      0.00      0.00         1\n",
            "         123       0.00      0.00      0.00         1\n",
            "         124       0.00      0.00      0.00         1\n",
            "         125       0.00      0.00      0.00         1\n",
            "         126       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         1\n",
            "         129       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         1\n",
            "         131       0.00      0.00      0.00         1\n",
            "         132       0.00      0.00      0.00         1\n",
            "         133       0.00      0.00      0.00         1\n",
            "         134       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00         1\n",
            "         136       0.00      0.00      0.00         1\n",
            "         137       0.00      0.00      0.00         1\n",
            "         138       0.00      0.00      0.00         1\n",
            "         139       0.00      0.00      0.00         1\n",
            "         140       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00         1\n",
            "         142       0.00      0.00      0.00         1\n",
            "         143       0.00      0.00      0.00         1\n",
            "         144       0.00      0.00      0.00         1\n",
            "         145       0.00      0.00      0.00         1\n",
            "         146       0.00      0.00      0.00         1\n",
            "         147       0.00      0.00      0.00         1\n",
            "         148       0.00      0.00      0.00         1\n",
            "         149       0.00      0.00      0.00         1\n",
            "         150       0.00      0.00      0.00         1\n",
            "         151       0.00      0.00      0.00         1\n",
            "         152       0.00      0.00      0.00         1\n",
            "         153       0.00      0.00      0.00         1\n",
            "         154       0.00      0.00      0.00         1\n",
            "         155       0.00      0.00      0.00         1\n",
            "         156       0.00      0.00      0.00         1\n",
            "         157       0.00      0.00      0.00         1\n",
            "         158       0.00      0.00      0.00         1\n",
            "         159       0.00      0.00      0.00         1\n",
            "         160       0.00      0.00      0.00         1\n",
            "         161       0.00      0.00      0.00         1\n",
            "         162       0.00      0.00      0.00         1\n",
            "         163       0.00      0.00      0.00         1\n",
            "         164       0.00      0.00      0.00         1\n",
            "         165       0.00      0.00      0.00         1\n",
            "         166       0.00      0.00      0.00         1\n",
            "         167       0.00      0.00      0.00         1\n",
            "         168       0.00      0.00      0.00         1\n",
            "         169       0.00      0.00      0.00         1\n",
            "         170       0.00      0.00      0.00         1\n",
            "         171       0.00      0.00      0.00         1\n",
            "         172       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         1\n",
            "         174       0.00      0.00      0.00         1\n",
            "         175       0.00      0.00      0.00         1\n",
            "         176       0.00      0.00      0.00         1\n",
            "         177       0.00      0.00      0.00         1\n",
            "         178       0.00      0.00      0.00         1\n",
            "         179       0.00      0.00      0.00         1\n",
            "         180       0.00      0.00      0.00         1\n",
            "         181       0.00      0.00      0.00         1\n",
            "         182       0.00      0.00      0.00         1\n",
            "         183       0.00      0.00      0.00         1\n",
            "         184       0.00      0.00      0.00         1\n",
            "         185       0.00      0.00      0.00         1\n",
            "         186       0.00      0.00      0.00         1\n",
            "         187       0.00      0.00      0.00         1\n",
            "         188       0.00      0.00      0.00         1\n",
            "         189       0.00      0.00      0.00         1\n",
            "         190       0.00      0.00      0.00         1\n",
            "         191       0.00      0.00      0.00         1\n",
            "         192       0.00      0.00      0.00         1\n",
            "         193       0.00      0.00      0.00         1\n",
            "         194       0.00      0.00      0.00         1\n",
            "         195       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         1\n",
            "         197       0.00      0.00      0.00         1\n",
            "         198       0.00      0.00      0.00         1\n",
            "         199       0.00      0.00      0.00         1\n",
            "         200       0.00      0.00      0.00         1\n",
            "         201       0.00      0.00      0.00         1\n",
            "         202       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         1\n",
            "         204       0.00      0.00      0.00         1\n",
            "         205       0.00      0.00      0.00         1\n",
            "         206       0.00      0.00      0.00         1\n",
            "         207       0.00      0.00      0.00         1\n",
            "         208       0.00      0.00      0.00         1\n",
            "         209       0.00      0.00      0.00         1\n",
            "         210       0.00      0.00      0.00         1\n",
            "         211       0.00      0.00      0.00         1\n",
            "         212       0.00      0.00      0.00         1\n",
            "         213       0.00      0.00      0.00         1\n",
            "         214       0.00      0.00      0.00         1\n",
            "         215       0.00      0.00      0.00         1\n",
            "         216       0.00      0.00      0.00         1\n",
            "         217       0.00      0.00      0.00         1\n",
            "         218       0.00      0.00      0.00         1\n",
            "         219       0.00      0.00      0.00         1\n",
            "         220       0.00      0.00      0.00         1\n",
            "         221       0.00      0.00      0.00         1\n",
            "         222       0.00      0.00      0.00         1\n",
            "         223       0.00      0.00      0.00         1\n",
            "         224       0.00      0.00      0.00         1\n",
            "         225       0.00      0.00      0.00         1\n",
            "         226       0.00      0.00      0.00         1\n",
            "         227       0.00      0.00      0.00         1\n",
            "         228       0.00      0.00      0.00         1\n",
            "         229       0.00      0.00      0.00         1\n",
            "         230       0.00      0.00      0.00         1\n",
            "         231       0.00      0.00      0.00         1\n",
            "         232       0.00      0.00      0.00         1\n",
            "         233       0.00      0.00      0.00         1\n",
            "         234       0.00      0.00      0.00         1\n",
            "         235       0.00      0.00      0.00         1\n",
            "         236       0.00      0.00      0.00         1\n",
            "         237       0.00      0.00      0.00         1\n",
            "         238       0.00      0.00      0.00         1\n",
            "         239       0.00      0.00      0.00         1\n",
            "         240       0.00      0.00      0.00         1\n",
            "         241       0.00      0.00      0.00         1\n",
            "         242       0.00      0.00      0.00         1\n",
            "         243       0.00      0.00      0.00         1\n",
            "         244       0.00      0.00      0.00         1\n",
            "         245       0.00      0.00      0.00         1\n",
            "         246       0.00      0.00      0.00         1\n",
            "         247       0.00      0.00      0.00         1\n",
            "         248       0.00      0.00      0.00         1\n",
            "         249       0.00      0.00      0.00         1\n",
            "         250       0.00      0.00      0.00         1\n",
            "         251       0.00      0.00      0.00         1\n",
            "         252       0.00      0.00      0.00         1\n",
            "         253       0.00      0.00      0.00         1\n",
            "         254       0.00      0.00      0.00         1\n",
            "         255       0.14      1.00      0.25         1\n",
            "         256       0.00      0.00      0.00         1\n",
            "         257       0.00      0.00      0.00         1\n",
            "         258       0.00      0.00      0.00         1\n",
            "         259       0.00      0.00      0.00         1\n",
            "         260       0.00      0.00      0.00         1\n",
            "         261       0.00      0.00      0.00         1\n",
            "         262       0.00      0.00      0.00         1\n",
            "         263       0.00      0.00      0.00         1\n",
            "         264       0.00      0.00      0.00         1\n",
            "         265       0.00      0.00      0.00         1\n",
            "         266       0.00      0.00      0.00         1\n",
            "         267       0.00      0.00      0.00         1\n",
            "         268       0.00      0.00      0.00         1\n",
            "         269       0.00      0.00      0.00         1\n",
            "         270       0.00      0.00      0.00         1\n",
            "         271       0.00      0.00      0.00         1\n",
            "         272       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         274       0.00      0.00      0.00         1\n",
            "         275       0.00      0.00      0.00         1\n",
            "         276       0.00      0.00      0.00         1\n",
            "         277       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         1\n",
            "         279       0.00      0.00      0.00         1\n",
            "         280       0.00      0.00      0.00         1\n",
            "         281       0.00      0.00      0.00         1\n",
            "         282       0.00      0.00      0.00         1\n",
            "         283       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00         1\n",
            "         285       0.00      0.00      0.00         1\n",
            "         286       0.00      0.00      0.00         1\n",
            "         287       0.00      0.00      0.00         1\n",
            "         288       0.00      0.00      0.00         1\n",
            "         289       0.00      0.00      0.00         1\n",
            "         290       0.00      0.00      0.00         1\n",
            "         291       0.00      0.00      0.00         1\n",
            "         292       0.00      0.00      0.00         1\n",
            "         293       0.00      0.00      0.00         1\n",
            "         294       0.00      0.00      0.00         1\n",
            "         295       0.00      0.00      0.00         1\n",
            "         296       0.00      0.00      0.00         1\n",
            "         297       0.00      0.00      0.00         1\n",
            "         298       0.00      0.00      0.00         1\n",
            "         299       0.00      0.00      0.00         1\n",
            "         300       0.00      0.00      0.00         1\n",
            "         301       0.00      0.00      0.00         1\n",
            "         302       0.00      0.00      0.00         1\n",
            "         303       0.00      0.00      0.00         1\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         306       0.00      0.00      0.00         1\n",
            "         307       0.00      0.00      0.00         1\n",
            "         308       0.00      0.00      0.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       0.00      0.00      0.00         1\n",
            "         312       0.00      0.00      0.00         1\n",
            "         313       0.00      0.00      0.00         1\n",
            "         314       0.00      0.00      0.00         1\n",
            "         315       0.00      0.00      0.00         1\n",
            "         316       0.00      0.00      0.00         1\n",
            "         317       0.00      0.00      0.00         1\n",
            "         318       0.00      0.00      0.00         1\n",
            "         319       0.00      0.00      0.00         1\n",
            "         320       0.00      0.00      0.00         1\n",
            "         321       0.00      0.00      0.00         1\n",
            "         322       0.00      0.00      0.00         1\n",
            "         323       0.00      0.00      0.00         1\n",
            "         324       0.00      0.00      0.00         1\n",
            "         325       0.00      0.00      0.00         1\n",
            "         326       0.00      0.00      0.00         1\n",
            "         327       0.00      0.00      0.00         1\n",
            "         328       0.00      0.00      0.00         1\n",
            "         329       0.00      0.00      0.00         1\n",
            "         330       0.00      0.00      0.00         1\n",
            "         331       0.00      0.00      0.00         1\n",
            "         332       0.00      0.00      0.00         1\n",
            "         333       0.00      0.00      0.00         1\n",
            "         334       0.00      0.00      0.00         1\n",
            "         335       0.00      0.00      0.00         1\n",
            "         336       0.00      0.00      0.00         1\n",
            "         337       0.00      0.00      0.00         1\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         341       0.00      0.00      0.00         1\n",
            "         342       0.00      0.00      0.00         1\n",
            "         343       0.00      0.00      0.00         1\n",
            "         344       0.00      0.00      0.00         1\n",
            "         345       0.00      0.00      0.00         1\n",
            "         346       0.00      0.00      0.00         1\n",
            "         347       0.00      0.00      0.00         1\n",
            "         348       0.00      0.00      0.00         1\n",
            "         349       0.00      0.00      0.00         1\n",
            "         350       0.00      0.00      0.00         1\n",
            "         351       0.00      0.00      0.00         1\n",
            "         352       0.00      0.00      0.00         1\n",
            "         353       0.00      0.00      0.00         1\n",
            "         354       0.00      0.00      0.00         1\n",
            "         355       0.00      0.00      0.00         1\n",
            "         356       0.00      0.00      0.00         1\n",
            "         357       0.00      0.00      0.00         1\n",
            "         358       0.00      0.00      0.00         1\n",
            "         359       0.00      0.00      0.00         1\n",
            "         360       0.00      0.00      0.00         1\n",
            "         361       0.00      0.00      0.00         1\n",
            "         362       0.00      0.00      0.00         1\n",
            "         363       0.00      0.00      0.00         1\n",
            "         364       0.00      0.00      0.00         1\n",
            "         365       0.00      0.00      0.00         1\n",
            "         366       0.00      0.00      0.00         1\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00         1\n",
            "         369       0.00      0.00      0.00         1\n",
            "         370       0.00      0.00      0.00         1\n",
            "         371       0.00      0.00      0.00         1\n",
            "         372       0.00      0.00      0.00         1\n",
            "         373       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         1\n",
            "         375       0.00      0.00      0.00         1\n",
            "         376       0.00      0.00      0.00         1\n",
            "         377       0.00      0.00      0.00         1\n",
            "         378       0.00      0.00      0.00         1\n",
            "         379       0.00      0.00      0.00         1\n",
            "         380       0.00      0.00      0.00         1\n",
            "         381       0.00      0.00      0.00         1\n",
            "         382       0.00      0.00      0.00         1\n",
            "         383       0.00      0.00      0.00         1\n",
            "         384       0.00      0.00      0.00         1\n",
            "         385       0.00      0.00      0.00         1\n",
            "         386       0.00      0.00      0.00         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         388       0.00      0.00      0.00         1\n",
            "         389       0.00      0.00      0.00         1\n",
            "         390       0.00      0.00      0.00         1\n",
            "         391       0.00      0.00      0.00         1\n",
            "         392       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         1\n",
            "         395       0.00      0.00      0.00         1\n",
            "         396       0.00      0.00      0.00         1\n",
            "         397       0.00      0.00      0.00         1\n",
            "         398       0.00      0.00      0.00         1\n",
            "         399       0.00      0.00      0.00         1\n",
            "         400       0.00      0.00      0.00         1\n",
            "         401       0.00      0.00      0.00         1\n",
            "         402       0.00      0.00      0.00         1\n",
            "         403       0.00      0.00      0.00         1\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00         1\n",
            "         407       0.00      0.00      0.00         1\n",
            "         408       0.00      0.00      0.00         1\n",
            "         409       0.00      0.00      0.00         1\n",
            "         410       0.00      0.00      0.00         1\n",
            "         411       0.00      0.00      0.00         1\n",
            "         412       0.00      0.00      0.00         1\n",
            "         413       0.00      0.00      0.00         1\n",
            "         414       0.00      0.00      0.00         1\n",
            "         415       0.00      0.00      0.00         1\n",
            "         416       0.00      0.00      0.00         1\n",
            "         417       0.00      0.00      0.00         1\n",
            "         418       0.00      0.00      0.00         1\n",
            "         419       0.00      0.00      0.00         1\n",
            "         420       0.00      0.00      0.00         1\n",
            "         421       0.00      0.00      0.00         1\n",
            "         422       0.00      0.00      0.00         1\n",
            "         423       0.00      0.00      0.00         1\n",
            "         424       0.00      0.00      0.00         1\n",
            "         425       0.00      0.00      0.00         1\n",
            "         426       0.00      0.00      0.00         1\n",
            "         427       0.00      0.00      0.00         1\n",
            "         428       0.00      0.00      0.00         1\n",
            "         429       0.00      0.00      0.00         1\n",
            "         430       0.00      0.00      0.00         1\n",
            "         431       0.00      0.00      0.00         1\n",
            "         432       0.00      0.00      0.00         1\n",
            "         433       0.00      0.00      0.00         1\n",
            "         434       0.00      0.00      0.00         1\n",
            "         435       0.00      0.00      0.00         1\n",
            "         436       0.00      0.00      0.00         1\n",
            "         437       0.00      0.00      0.00         1\n",
            "         438       0.00      0.00      0.00         1\n",
            "         439       0.00      0.00      0.00         1\n",
            "         440       0.00      0.00      0.00         1\n",
            "         441       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         443       0.00      0.00      0.00         1\n",
            "         444       0.00      0.00      0.00         1\n",
            "         445       0.00      0.00      0.00         1\n",
            "         446       0.00      0.00      0.00         1\n",
            "         447       0.00      0.00      0.00         1\n",
            "         448       0.00      0.00      0.00         1\n",
            "         449       0.00      0.00      0.00         1\n",
            "         450       0.00      0.00      0.00         1\n",
            "         451       0.00      0.00      0.00         1\n",
            "         452       0.00      0.00      0.00         1\n",
            "         453       0.00      0.00      0.00         1\n",
            "         454       0.00      0.00      0.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         456       0.00      0.00      0.00         1\n",
            "         457       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         1\n",
            "         459       0.00      0.00      0.00         1\n",
            "         460       0.00      0.00      0.00         1\n",
            "         461       0.00      0.00      0.00         1\n",
            "         462       0.00      0.00      0.00         1\n",
            "         463       0.00      0.00      0.00         1\n",
            "         464       0.00      0.00      0.00         1\n",
            "         465       0.00      0.00      0.00         1\n",
            "         466       0.00      0.00      0.00         1\n",
            "         467       0.00      0.00      0.00         1\n",
            "         468       0.00      0.00      0.00         1\n",
            "         469       0.00      0.00      0.00         1\n",
            "         470       0.00      0.00      0.00         1\n",
            "         471       0.00      0.00      0.00         1\n",
            "         472       0.00      0.00      0.00         1\n",
            "         473       0.00      0.00      0.00         1\n",
            "         474       0.00      0.00      0.00         1\n",
            "         483       0.00      0.00      0.00         0\n",
            "         502       0.00      0.00      0.00         0\n",
            "         503       0.00      0.00      0.00         0\n",
            "         509       0.00      0.00      0.00         0\n",
            "         512       0.00      0.00      0.00         0\n",
            "         517       0.00      0.00      0.00         0\n",
            "         525       0.00      0.00      0.00         0\n",
            "         527       0.00      0.00      0.00         0\n",
            "         528       0.00      0.00      0.00         0\n",
            "         529       0.00      0.00      0.00         0\n",
            "         530       0.00      0.00      0.00         0\n",
            "         533       0.00      0.00      0.00         0\n",
            "         535       0.00      0.00      0.00         0\n",
            "         537       0.00      0.00      0.00         0\n",
            "         542       0.00      0.00      0.00         0\n",
            "         543       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.00       475\n",
            "   macro avg       0.00      0.00      0.00       475\n",
            "weighted avg       0.00      0.00      0.00       475\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrpGJMAbufKl",
        "outputId": "08fc2c51-be33-4ce0-8935-a0cc5aaba273"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"./trainingData.csv\", sep=';',header=None)\n",
        "print(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                      0\n",
            "0     WAP001,WAP002,WAP003,WAP004,WAP005,WAP006,WAP0...\n",
            "1     100,100,100,100,100,100,100,100,100,100,100,10...\n",
            "2     100,100,100,100,100,100,100,100,100,100,100,10...\n",
            "3     100,100,100,100,100,100,100,-97,100,100,100,10...\n",
            "4     100,100,100,100,100,100,100,100,100,100,100,10...\n",
            "...                                                 ...\n",
            "5869  100,100,100,100,100,100,100,100,100,100,-89,10...\n",
            "5870  100,100,100,100,100,-88,100,100,100,100,100,10...\n",
            "5871  100,100,100,100,100,100,100,100,100,100,100,10...\n",
            "5872  100,100,100,100,100,-89,100,100,100,100,100,10...\n",
            "5873  100,100,100,100,100,100,100,100,100,100,100,10...\n",
            "\n",
            "[5874 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}