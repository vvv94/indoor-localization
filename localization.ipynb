{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "localization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4oXknE3gpAXt3VRy7oVwq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvv94/indoor-localization/blob/main/localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cblM3xFjcnln"
      },
      "source": [
        "Installing required Packages and import Python Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVyNWaZZfsE"
      },
      "source": [
        "from time import sleep"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEum84dOccHb"
      },
      "source": [
        "Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUIqo_7WWyk"
      },
      "source": [
        "!rm -r sample_data dataset README.rtf Data.csv >/dev/null 2>&1\n",
        "!mkdir -p dataset >/dev/null 2>&1\n",
        "# WIFI Data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=11El95X9yFnGqWhj7e9IulhYLN8SalJg8' -O ./dataset/AP_Data.csv -q\n",
        "# Beacons Data\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hDNQROcOcSDsbIUaVkTRdmYIrN81pNkA' -ÎŸ ./dataset/Beacon_Data.csv -q\n",
        "!mkdir -p results >/dev/null 2>&1"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkAtRY8cuzA"
      },
      "source": [
        "Load Dataset into a Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNb6H5zL6uVe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "from functools import reduce\n",
        "\n",
        "data = pd.read_csv(\"./dataset/AP_Data.csv\", sep=';',index_col=0)\n",
        "\n",
        "# Extract Location of AP's\n",
        "AP_Locs = list(zip(data.iloc[0].values.astype(str).tolist(), data.iloc[1].values.astype(str).tolist()))\n",
        "AP_Locs = AP_Locs[:len(AP_Locs)-2]\n",
        "for idx,coords in enumerate(AP_Locs):\n",
        "  x,y = coords\n",
        "  x = x.replace(',', '.')\n",
        "  y = y.replace(',', '.')\n",
        "  AP_Locs[idx] = (x,y)\n",
        "AP_Locs = np.asarray(AP_Locs,dtype=np.float64)\n",
        "\n",
        "\n",
        "# Extract Location of AP's\n",
        "Points_Locs = list(zip(data.iloc[2:,11].values.astype(str).tolist(),data.iloc[2:,12].values.astype(str).tolist()))\n",
        "for idx,coords in enumerate(Points_Locs):\n",
        "  x,y = coords\n",
        "  x = x.replace(',', '.')\n",
        "  y = y.replace(',', '.')\n",
        "  Points_Locs[idx] = (x,y)\n",
        "Points_Locs = np.asarray(Points_Locs,dtype=np.float64)\n",
        "x, y = np.asarray(Points_Locs[:,0],dtype=str), np.asarray(Points_Locs[:,1],dtype=str)\n",
        "Points_Locs = np.asarray([x[i]+'_'+y[i] for i in range(len(x))])\n",
        "\n",
        "# Extract RSS of Points\n",
        "RSS_Values = np.asarray(data.iloc[2:,0:11].values.astype(int).tolist(),np.int64)\n",
        "RSS_Values = scale(RSS_Values)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(RSS_Values,Points_Locs,test_size=0.2,random_state=2020)\n",
        "Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
        "Y_test = Y_test.reshape(Y_test.shape[0],1)"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYuw4gyAjBPH",
        "outputId": "44912166-60f6-476a-825f-dd767e6b0271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#clf =DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, max_leaf_nodes=None)\n",
        "#fit = clf.fit(X_train,Y_train)\n",
        "#y_pre =fit.predict(X_test)\n",
        "#print(classification_report(Y_test,y_pre))\n",
        "\n",
        "kf = KFold(n_splits=10,random_state=4, shuffle=False)\n",
        "\n",
        "##Instantiatie my chosen model \n",
        "#clf = DecisionTreeClassifier(criterion='gini', max_depth=25, min_samples_split=2, min_samples_leaf=1, max_features=None, max_leaf_nodes=None)\n",
        "clf = KNeighborsClassifier(5,weights='distance', p=2)\n",
        "\n",
        "kf = KFold(n_splits=10,random_state=2020, shuffle=True)\n",
        "for k, (train_index, test_index) in enumerate(kf.split(RSS_Values)):\n",
        " \n",
        "    X_train, X_test = RSS_Values[train_index], RSS_Values[test_index]\n",
        "    y_train, y_test = Points_Locs[train_index], Points_Locs[test_index]\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))\n",
        "print(\"Fold,Mean \",k, clf.score(X_test, y_test).mean())\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[fold 0] score: 0.00000\n",
            "[fold 1] score: 0.00000\n",
            "[fold 2] score: 0.00000\n",
            "[fold 3] score: 0.00000\n",
            "[fold 4] score: 0.00000\n",
            "[fold 5] score: 0.00000\n",
            "[fold 6] score: 0.00000\n",
            "[fold 7] score: 0.00000\n",
            "[fold 8] score: 0.00000\n",
            "[fold 9] score: 0.00000\n",
            "Fold,Mean  9 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340Y50V2a6y6"
      },
      "source": [
        "Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrpGJMAbufKl"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "class Network:\n",
        "\n",
        "  def __init__(self,input_size=11, num_classes=128):\n",
        "\n",
        "      self.input_size = input_size\n",
        "      self.output_size = num_classes\n",
        "      self.dropout = 0.1\n",
        "\n",
        "  def classifier(self):\n",
        "\n",
        "      classifier = Sequential()\n",
        "      classifier.add(Dense(units = 500, activation='relu', kernel_initializer='glorot_uniform', input_dim=self.input_size))\n",
        "      classifier.add(Dropout(self.dropout))\n",
        "      classifier.add(Dense(units = 100, activation='relu', kernel_initializer = 'glorot_uniform'))\n",
        "      classifier.add(Dropout(self.dropout))\n",
        "      classifier.add(Dense(units = 200, activation='relu', kernel_initializer = 'glorot_uniform'))\n",
        "      classifier.add(Dropout(self.dropout))\n",
        "      classifier.add(Dense(units = 50, activation='relu', kernel_initializer = 'glorot_uniform'))\n",
        "      classifier.add(Dropout(self.dropout))\n",
        "      classifier.add(Dense(units = self.output_size, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "      classifier.compile(optimizer = Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "      model = classifier\n",
        "\n",
        "      return model"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snGapyy0i-md"
      },
      "source": [
        "# Load Model\n",
        "model = Network(input_size=128, num_classes=128).classifier()"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g29n-fD7eKh7"
      },
      "source": [
        "# Train Model\n",
        "model.fit(train_data, labels, epochs=20, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OZg_8hzi837"
      },
      "source": [
        "# Evaluate Model\n",
        "loss, acc = c.evaluate(test_features, test_labels)\n",
        "print loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRMeK5kyQ8Om"
      },
      "source": [
        "More"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdU2kQOIb0JU"
      },
      "source": [
        "'''Project Parameters'''\n",
        "DATASET_DIR = \"./dataset/\"\n",
        "RESULTS_DIR = \"./results\"\n",
        "DATASET_NAMES = [   \n",
        "'P_SA',             # RSS values between the smartphone and 11 WiFi APs\n",
        "'P_SA_Signatures',  # RSS values between the smartphone and 11 WiFi APs for calibration points (signatures)\n",
        "'P_SA_Tests',       # RSS values between the smartphone and 11 WiFi APs for test points\n",
        "'APLocs',           # x and y coordinates of 11 APs with RSS values stored in P_SA\n",
        "\n",
        "'P_SB',             # RSS values between the smartphone and 11 WiFi APs\n",
        "'P_SB_Signatures',  # RSS values between the smartphone and 46 Bluetooth beacons for calibration points (signatures)\n",
        "'P_SB_Tests',       # RSS values between the smartphone and 46 Bluetooth beacons for test points\n",
        "'BeaconLocs',       # x and y coordinates of 46 Bluetooth beacons with RSS values stored in P_SB\n",
        "\n",
        "'MeasLocs',         # x and y coordinates of the smartphone with RSS values stored in P_SA and P_SB\n",
        "'SignatureLocs',    # x and y coordinates of calibration points (signatures) with RSS values stored in P_SA_Signature and P_SB_Signature\n",
        "'TestLocs',         # x and y coordinates of test points with RSS values stored in P_SA_Tests and P_SB_Tests\n",
        "'P_Signatures',     # combination of RSS values for calibration points (signatures) - P_SA_Signature and P_SB_Signature\n",
        "'P_Tests',          # combination of RSS values for test points - P_SA_Tests and P_SB_Tests\n",
        "]\n",
        "\n",
        "def pre_process(data_name):\n",
        "\n",
        "  _colnames = []\n",
        "  if data_name == \"P_SA\":\n",
        "    _colnames=['WF1 RSS', 'WF2 RSS', 'WF3 RSS', 'WF4 RSS','WF5 RSS', 'WF6 RSS', 'WF7 RSS', 'WF8 RSS','WF9 RSS', 'WF10 RSS', 'WF11 RSS']\n",
        "    data = pd.read_csv(\"./dataset/P_SA.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SA_Signatures\":\n",
        "    _colnames=['WF1 RSS Sig.', 'WF2 RSS Sig.', 'WF3 RSS Sig.', 'WF4 RSS Sig.','WF5 RSS Sig.', 'WF6 RSS Sig.', 'WF7 RSS Sig.', 'WF8 RSS Sig.','WF9 RSS Sig.', 'WF10 RSS Sig.', 'WF11 RSS Sig.']\n",
        "    data = pd.read_csv(\"./dataset/P_SA_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SA_Tests\":\n",
        "    _colnames=['WF1 RSS Test', 'WF2 RSS Test', 'WF3 RSS Test', 'WF4 RSS Test','WF5 RSS Test', 'WF6 RSS Test', 'WF7 RSS Test', 'WF8 RSS Test','WF9 RSS Test', 'WF10 RSS Test', 'WF11 RSS Test']\n",
        "    data = pd.read_csv(\"./dataset/P_SA_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"APLocs\":\n",
        "    _colnames=['AP Coordinates', 'RSS Value in P_SA']\n",
        "    data = pd.read_csv(\"./dataset/APLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB_Signatures\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS Sig.') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_SB_Tests\":\n",
        "    [_colnames.append('BT'+str(i)+' RSS Test') for i in range(1,47)]\n",
        "    data = pd.read_csv(\"./dataset/P_SB_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"BeaconLocs\":\n",
        "    _colnames=['BT Coordinates', 'RSS Value in P_SB']\n",
        "    data = pd.read_csv(\"./dataset/BeaconLocs.csv\", sep='\\t',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"MeasLocs\":\n",
        "    _colnames=['Smartphone Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/MeasLocs.csv\", sep='\t',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"SignatureLocs\":\n",
        "    _colnames=['Calibration Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/SignatureLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"TestLocs\":\n",
        "    _colnames=['Test Coordinates', 'RSS in P_SA and P_SB']\n",
        "    data = pd.read_csv(\"./dataset/TestLocs.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_Signatures\":\n",
        "    [_colnames.append('WF'+str(i)+' RSS Sign.') for i in range(1,12)]\n",
        "    [_colnames.append('BT'+str(i)+' RSS Sign.') for i in range(12,58)]\n",
        "    data = pd.read_csv(\"./dataset/P_Signatures.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  elif data_name == \"P_Tests\":\n",
        "    [_colnames.append('WF'+str(i)+' RSS Test') for i in range(1,12)]\n",
        "    [_colnames.append('BT'+str(i)+' RSS Test') for i in range(12,58)]\n",
        "    data = pd.read_csv(\"./dataset/P_Tests.csv\", sep=';',names=_colnames,header=None)\n",
        "\n",
        "  else:\n",
        "    data = []\n",
        "\n",
        "  return data\n",
        "\n",
        "data = [pre_process(name) for idx, name in enumerate(DATASET_NAMES)]\n",
        "dataset1 = [data[0], data[1], data[2], data[3]]\n",
        "dataset2 = [data[4], data[5], data[6], data[7]]\n",
        "dataset3 = [data[8], data[9], data[10], data[11], data[12]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}